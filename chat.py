import os
import asyncio
import streamlit as st
from langchain.chains import RetrievalQA
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate

# éåŒæœŸé–¢æ•°ã‚’åŒæœŸçš„ã«å®Ÿè¡Œã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
def run_async(func, *args, **kwargs):
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    return loop.run_until_complete(func(*args, **kwargs))

# FAISSãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿
def load_db(embeddings):
    return FAISS.load_local('faiss_store', embeddings, allow_dangerous_deserialization=True)

# Streamlitãƒšãƒ¼ã‚¸åˆæœŸåŒ–
def init_page():
    st.set_page_config(
        page_title='ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ',
        page_icon='ğŸ§‘â€ğŸ’»',
    )

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    init_page()

    # Google APIã‚­ãƒ¼ã®å–å¾—
    api_key = os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # Embeddingsã¨LLMã®åˆæœŸåŒ–ï¼ˆåŒæœŸçš„ã«æ‰±ãˆã‚‹å ´åˆã¯ãã®ã¾ã¾ï¼‰
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=api_key
    )

    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        temperature=0.0,
        max_retries=2,
        google_api_key=api_key
    )

    db = load_db(embeddings)

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å®šç¾©
    prompt_template = """
    ã‚ãªãŸã¯ã€ã€Œå¥åº·å±æ©Ÿç®¡ç†ã«æºã‚ã‚‹åœ°æ–¹å…¬å…±å›£ä½“ç­‰æ©Ÿé–¢è·å“¡ã€å°‚ç”¨ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚
    èƒŒæ™¯æƒ…å ±ã‚’å‚è€ƒã«ã€è³ªå•ã«å¯¾ã—ã¦æœŸé–“è·å“¡ã«ãªã‚Šãã£ã¦ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã„ã€‚

    å¥åº·å±æ©Ÿç®¡ç†ã«æºã‚ã‚‹åœ°æ–¹å…¬å…±å›£ä½“ç­‰æ©Ÿé–¢ã«å…¨ãé–¢ä¿‚ã®ãªã„è³ªå•ã¨æ€ã‚ã‚Œã‚‹è³ªå•ã«é–¢ã—ã¦ã¯ã€ã€Œå¥åº·å±æ©Ÿç®¡ç†ã«æºã‚ã‚‹åœ°æ–¹å…¬å…±å›£ä½“ç­‰æ©Ÿé–¢ã«é–¢ä¿‚ã™ã‚‹ã“ã¨ã«ã¤ã„ã¦èã„ã¦ãã ã•ã„ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

    ä»¥ä¸‹ã®èƒŒæ™¯æƒ…å ±ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚æƒ…å ±ãŒãªã‘ã‚Œã°ã€ãã®å†…å®¹ã«ã¤ã„ã¦ã¯è¨€åŠã—ãªã„ã§ãã ã•ã„ã€‚
    # èƒŒæ™¯æƒ…å ±
    {context}

    # è³ªå•
    {question}
    """

    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )

    # RetrievalQAãƒã‚§ãƒ¼ãƒ³ã®æ§‹ç¯‰
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=db.as_retriever(search_kwargs={"k": 2}),
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®å‡¦ç†
    if user_input := st.chat_input('è³ªå•ã—ã‚ˆã†ï¼'):
        # éå»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        with st.chat_message('user'):
            st.markdown(user_input)
        st.session_state.messages.append({"role": "user", "content": user_input})

        with st.chat_message('assistant'):
            with st.spinner('Gemini is typing ...'):
                # éåŒæœŸé–¢æ•°ã‚’åŒæœŸçš„ã«å®Ÿè¡Œ
                response = run_async(qa.invoke, user_input)
            st.markdown(response['result'])

            # å‚è€ƒå…ƒã®è¡¨ç¤º
            doc_urls = []
            for doc in response["source_documents"]:
                url = doc.metadata.get("source_url")
                if url and url not in doc_urls:
                    doc_urls.append(url)
                    st.markdown(f"å‚è€ƒå…ƒï¼š{url}")

        st.session_state.messages.append({"role": "assistant", "content": response["result"]})

# å®Ÿè¡Œ
if __name__ == "__main__":
    main()
